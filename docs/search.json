[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiayi He",
    "section": "",
    "text": "Here is the summary of me!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "My Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/hw1.html",
    "href": "blog/hw1.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nDescription of the experiment: The experiment was designed to test whether offering a matching grant affects charitable giving. Karlan and List collaborated with a liberal nonprofit organization and sent fundraising letters to over 50,000 prior donors. These letters were randomly assigned to either a control group or a treatment group.\nThe control group received a standard fundraising letter. The treatment group received a nearly identical letter, except it included an announcement that a “concerned member” would match their donation. Within the treatment group, there were further randomizations: donors received different match ratios (1:1, 2:1, or 3:1), different maximum matching amounts ($25,000, $50,000, $100,000, or unspecified), and different suggested donation amounts (equal to, 1.25×, or 1.5× their previous highest contribution).\nThis design allows the authors to estimate both the overall effect of matching gifts and to test whether larger match ratios or different framing elements lead to higher donations. This project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1.html#introduction",
    "href": "blog/hw1.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nDescription of the experiment: The experiment was designed to test whether offering a matching grant affects charitable giving. Karlan and List collaborated with a liberal nonprofit organization and sent fundraising letters to over 50,000 prior donors. These letters were randomly assigned to either a control group or a treatment group.\nThe control group received a standard fundraising letter. The treatment group received a nearly identical letter, except it included an announcement that a “concerned member” would match their donation. Within the treatment group, there were further randomizations: donors received different match ratios (1:1, 2:1, or 3:1), different maximum matching amounts ($25,000, $50,000, $100,000, or unspecified), and different suggested donation amounts (equal to, 1.25×, or 1.5× their previous highest contribution).\nThis design allows the authors to estimate both the overall effect of matching gifts and to test whether larger match ratios or different framing elements lead to higher donations. This project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1.html#data",
    "href": "blog/hw1.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset contains 50,083 observations and 51 variables. Each observation corresponds to one prior donor who received a fundraising letter. Individuals were randomly assigned to a control group or a treatment group. Within the treatment group, they were further randomly assigned to different match ratios, match thresholds, and suggested donation amounts. Key variables include: - treatment: indicates if the donor received a matching gift offer (1 = yes, 0 = no) - ratio2, ratio3: dummies for match ratios of 2:1 and 3:1 (baseline is 1:1) - size25, size50, size100: dummies for match maximum amount - askd1, askd2, askd3: suggested donation amounts based on prior donation - amount: actual amount donated - gave: binary variable indicating whether the donor gave (1 = yes, 0 = no)\nWe will use these variables to replicate the results in Karlan and List (2007).\n\nlibrary(haven)\nlibrary(margins)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(knitr)\n\n\n\n\n\n\n\n\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n\nVariable Definitions\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n#month since last donation \ntidy(t.test(mrm2 ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  -0.0137      13.0      13.0    -0.120   0.905    33394.   -0.238     0.211\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(mrm2 ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  13.0       0.0935   139.      0    \n2 treatment     0.0137    0.115      0.119   0.905\n\n# gender \ntidy(t.test(female ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  0.00755     0.283     0.275      1.75  0.0795    32451. -0.000889    0.0160\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(female ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.283     0.00350     80.7   0     \n2 treatment   -0.00755   0.00429     -1.76  0.0787\n\n# couple\ntidy(t.test(couple ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  0.00162    0.0930    0.0914     0.582   0.560    32439. -0.00383   0.00706\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(couple ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.0930    0.00226    41.1     0    \n2 treatment   -0.00162   0.00277    -0.584   0.559\n\n\n\n\nBalance Test Results\nTo evaluate whether the randomization successfully created comparable groups, we examine balance on several pre-treatment covariates: months since last donation (mrm2), gender (female), and couple status.\n\nFor months since last donation, the mean in the control group is 12.998 months and 13.012 months in the treatment group. The t-test yields a p-value of 0.9049, and the linear regression coefficient is not significant. This suggests no systematic difference between groups.\nFor gender, the control group has a female proportion of 28.3%, compared to 27.5% in the treatment group. The t-test gives a p-value of 0.0795. Although this is closer to significance, it is still above the 5% threshold, and the regression confirms no strong evidence of imbalance.\nFor couple status, the proportions are 9.30% (control) and 9.14% (treatment), with a p-value of 0.5604. No evidence suggests imbalance on this variable either.\n\nThese results confirm that the treatment assignment appears balanced across key demographic characteristics, supporting the validity of the experimental design."
  },
  {
    "objectID": "blog/hw1.html#experimental-results",
    "href": "blog/hw1.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n# Calculate donation rate by treatment group\ndonation_rate &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(response_rate = mean(gave == 1, na.rm = TRUE))\n\n# Rename group labels\ndonation_rate$treatment &lt;- factor(donation_rate$treatment, labels = c(\"Control\", \"Treatment\"))\n\n# Plot\nggplot(donation_rate, aes(x = treatment, y = response_rate)) +\n  geom_col(fill = \"steelblue\", width = 0.6) +\n  geom_text(aes(label = scales::percent(response_rate, accuracy = 0.1)), vjust = -0.5) +\n  ylim(0, max(donation_rate$response_rate) + 0.05) +\n  labs(\n    title = \"Proportion of Donors by Treatment Group\",\n    x = \"Group\",\n    y = \"Proportion Who Donated\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe bar plots display the donation response rate. For the control group, we have 1.8% of individual donated and for the treatment group, we have 2.2% of individuals donated. This suggests that individuals who received the matching donation offer were more likely to donate compared to those who did not.\n\ntidy(t.test(gave ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00418    0.0179    0.0220     -3.21 0.00133    36577. -0.00673  -0.00163\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(gave ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.0179    0.00110     16.2  4.78e-59\n2 treatment    0.00418   0.00135      3.10 1.93e- 3\n\n\nThe results show a statistically significant difference: individuals in the treatment group were more likely to donate than those in the control group. While the donation rates were low overall — roughly 1.8% in the control group versus 2.2% in the treatment group — the difference of about 0.4 percentage points is small but meaningful, and it is unlikely to have occurred by chance (p-value &lt; 0.01). This tells us something powerful about human behavior: even a subtle nudge, like telling people their donation will be matched, can influence their decision to give. People seem more willing to act charitably when they believe their contribution will have a greater impact. In this case, the match offer acted as a psychological motivator, reinforcing the value and urgency of giving.\n\ndata &lt;- data %&gt;%\n  mutate(treatment = ifelse(ratio &gt; 0, 1, 0))\n\n# Probit model\nprobit_model &lt;- glm(gave ~ treatment, family = binomial(link = \"probit\"), data = data)\n\ntidy(probit_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -2.10      0.0233    -90.1  0      \n2 treatment     0.0868    0.0279      3.11 0.00185\n\n\n\nmfx &lt;- margins(probit_model)\nsummary(mfx)\n\n    factor    AME     SE      z      p  lower  upper\n treatment 0.0043 0.0014 3.1044 0.0019 0.0016 0.0070\n\n\nThe estimated average marginal effect of the treatment assignment is approximately 0.0043, meaning that the presence of a matching offer increased the probability of donating by about 0.4 percentage points.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Filter treatment groups only\nmatch_data &lt;- data %&gt;%\n  filter(ratio %in% c(1, 2, 3)) %&gt;%\n  mutate(gave = ifelse(amount &gt; 0, 1, 0))\n\n# Split into groups\ngroup_1_1 &lt;- match_data %&gt;% filter(ratio == 1)\ngroup_2_1 &lt;- match_data %&gt;% filter(ratio == 2)\ngroup_3_1 &lt;- match_data %&gt;% filter(ratio == 3)\n\n# Run t-tests\nt_2_vs_1 &lt;- tidy(t.test(group_2_1$gave, group_1_1$gave))\nt_3_vs_1 &lt;- tidy(t.test(group_3_1$gave, group_1_1$gave))\n\nt_2_vs_1\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  0.00188    0.0226    0.0207     0.965   0.335    22225. -0.00194   0.00571\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\nt_3_vs_1\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  0.00198    0.0227    0.0207      1.02   0.310    22215. -0.00185   0.00582\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nNeither the $2:$1 nor the $3:$1 match rate significantly increases the probability of donation compared to the $1:$1 rate. This aligns with the authors’ interpretation on page 8 — that larger match ratios do not provide additional effectiveness. Our t-tests support this claim: p-values for both comparisons are well above conventional thresholds for statistical significance.\n\n# Use ratio1 as the base category\nmatch_data &lt;- match_data %&gt;%\n  mutate(\n    ratio2 = as.numeric(ratio == 2),\n    ratio3 = as.numeric(ratio == 3)\n  )\n\n# Regression using ratio2 and ratio3 (1:1 is base)\nmodel &lt;- glm(gave ~ ratio2 + ratio3, family = binomial(link = \"probit\"), data = match_data)\n\n# Show marginal effects\ntidy(margins(model))\n\n# A tibble: 2 × 5\n  term   estimate std.error statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ratio2  0.00191   0.00198     0.965   0.335\n2 ratio3  0.00201   0.00198     1.01    0.310\n\n\nThe estimated marginal effect for the 2:1 match is 0.0019 with a p-value of 0.335. The estimated marginal effect for the 3:1 match is 0.0020 with a p-value of 0.310. These results are not statistically significant, indicating that neither the 2:1 nor the 3:1 match leads to a higher probability of giving compared to the 1:1 match.\n\n# Direct from data\nresponse_rates &lt;- match_data %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(response_rate = mean(gave))\n\nrate_2_vs_1 &lt;- response_rates$response_rate[response_rates$ratio == 2] - \n               response_rates$response_rate[response_rates$ratio == 1]\n\nrate_3_vs_2 &lt;- response_rates$response_rate[response_rates$ratio == 3] - \n               response_rates$response_rate[response_rates$ratio == 2]\n\n# Run the probit model using ratio as a categorical variable\nmodel_factor &lt;- glm(gave ~ factor(ratio), family = binomial(link = \"probit\"), data = match_data)\n\nmfx &lt;- summary(margins(model_factor))\n\nmfx_diff_2_vs_1 &lt;- mfx$AME[mfx$factor == \"ratio2\"]\nmfx_diff_3_vs_2 &lt;- mfx$AME[mfx$factor == \"ratio3\"] - mfx$AME[mfx$factor == \"ratio2\"]\n\nlist(\n  direct_data_differences = list(\n    `2:1 vs 1:1` = rate_2_vs_1,\n    `3:1 vs 2:1` = rate_3_vs_2\n  ),\n  regression_marginal_effect_differences = list(\n    `2:1 vs 1:1` = mfx_diff_2_vs_1,\n    `3:1 vs 2:1` = mfx_diff_3_vs_2\n  )\n)\n\n$direct_data_differences\n$direct_data_differences$`2:1 vs 1:1`\n[1] 0.001884251\n\n$direct_data_differences$`3:1 vs 2:1`\n[1] 0.000100024\n\n\n$regression_marginal_effect_differences\n$regression_marginal_effect_differences$`2:1 vs 1:1`\n     ratio2 \n0.001884251 \n\n$regression_marginal_effect_differences$`3:1 vs 2:1`\n     ratio3 \n0.000100024 \n\n\nFrom both the raw data and the regression model, we find: - The difference in response rate between 2:1 and 1:1 is approximately 0.00188 (0.19 percentage points). - The difference in response rate between 3:1 and 2:1 is a mere 0.00010 (0.01 percentage points).\nThese values are extremely small, and consistent across both the direct calculation from raw data and the marginal effects estimated from the probit model. The response rate differences between 1:1, 2:1, and 3:1 match ratios are economically negligible and statistically insignificant. This strongly supports the conclusion drawn in Karlan and List (2007):\n&gt; “Larger match ratios—$3:$1 and $2:$1—relative to smaller match ratios ($1:$1) had no additional impact.” Increasing the match ratio beyond 1:1 does not improve donor response rates and may not be a cost-effective strategy for fundraisers.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\ntidy(t.test(amount ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   -0.154     0.813     0.967     -1.92  0.0551    36216.   -0.311   0.00334\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(amount ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.813    0.0674     12.1  1.84e-33\n2 treatment      0.154    0.0826      1.86 6.28e- 2\n\n\nThe result from t-test, we have mean in control group is 0.81 and treatment group is 0.97, p-value is 0.055. The avg donation amount is slightly higher in the treatment group means that matching grants may slightly increase donation amounts but the main thing is to give people the incentive to give not on how much thet give.\n\n# Filter for donors only\ndonors_only &lt;- data %&gt;%\n  filter(amount &gt; 0)\n\n# Run OLS regression on positive donations\nmodel_conditional &lt;- lm(amount ~ treatment, data = donors_only)\n\ntidy(model_conditional)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    45.5       2.42    18.8   5.47e-68\n2 treatment      -1.67      2.87    -0.581 5.61e- 1\n\n\nFrom the output, the intercepts reflects the average donation amount in the control group. And the coefficient on treatment shows the average increase (or decrease) in donation amount among donors. This coefficient does not have a clear causal interpretation since we are limited the condition to gave =1 and the selection bias exists since the treatment may impact people who donates in the first place. Thus, this regression cannot identify the causal effect of treatment on donation size.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "blog/hw1.html#simulation-experiment",
    "href": "blog/hw1.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\n\nThe simulation illustrates the Law of Large Numbers in action. Initially, the cumulative average of the simulated differences is noisy and jumps around, but as the number of draws increases, it converges steadily toward the true mean difference of 0.004.This demonstrates how, in repeated sampling, the sample average becomes a reliable estimator of the population average — the foundational idea behind statistical inference methods like the t-test used earlier in this blog.It reinforces why even small effects (like a 0.004 difference in donation likelihood) can be detected reliably with large samples.\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\nThis simulation illustrates the Central Limit Theorem using differences in simulated donation probabilities between treatment and control groups. - With a small sample size (n = 50), the sampling distribution of the difference in means is noisy and not very symmetric. - As the sample size increases (n = 200, 500, 1000), the distributions become tighter, smoother, and more centered around the true mean difference - In all cases, zero is near the center, not in the tail — suggesting no systematic bias in our estimator. This demonstrates why, in the real-world Karlan & List field experiment (with over 50,000 subjects!), we can reliably detect even small differences in donation behavior. The CLT ensures that the sampling distribution of the mean is approximately normal and centered, allowing valid statistical inference."
  },
  {
    "objectID": "blog/hw2.html",
    "href": "blog/hw2.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(tidyr)\n\n\nblueprinty &lt;- read.csv(\"blueprinty.csv\")\nhead(blueprinty)\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\nairbnb &lt;- read.csv(\"airbnb.csv\")\nhead(airbnb)\n\n  X   id days last_scraped host_since       room_type bathrooms bedrooms price\n1 1 2515 3130     4/2/2017   9/6/2008    Private room         1        1    59\n2 2 2595 3127     4/2/2017   9/9/2008 Entire home/apt         1        0   230\n3 3 3647 3050     4/2/2017 11/25/2008    Private room         1        1   150\n4 4 3831 3038     4/2/2017  12/7/2008 Entire home/apt         1        1    89\n5 5 4611 3012     4/2/2017   1/2/2009    Private room        NA        1    39\n6 6 5099 2981     4/2/2017   2/2/2009 Entire home/apt         1        1   212\n  number_of_reviews review_scores_cleanliness review_scores_location\n1               150                         9                      9\n2                20                         9                     10\n3                 0                        NA                     NA\n4               116                         9                      9\n5                93                         9                      8\n6                60                         9                      9\n  review_scores_value instant_bookable\n1                   9                f\n2                   9                f\n3                  NA                f\n4                   9                f\n5                   9                t\n6                   9                f\n\n\n\n# compare mean\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_patents = mean(patents),\n    sd_patents = sd(patents),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Table 1. Average Number of Patents by Customer Status\")\n\n\nTable 1. Average Number of Patents by Customer Status\n\n\niscustomer\nmean_patents\nsd_patents\ncount\n\n\n\n\n0\n3.473013\n2.225060\n1019\n\n\n1\n4.133056\n2.546846\n481\n\n\n\n\n# Compare Histogram\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30)+\n  labs(title = \"Patent Counts by Customer Status\",\n       x = \"Number of Patents\",\n       fill = \"Customer (1=Yes, 0=No)\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nExplanation: Blueprinty customers have a higher average number of patents(4.133) compare to non-customer(3.473) and from the histogram distribution, we can clearly observer that the Blueprinty customer tend to be more represented in the higher end of patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_age = round(mean(age), 1),\n    sd_age = round(sd(age), 2),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Table 2. Average Age by Customer Status\")\n\n\nTable 2. Average Age by Customer Status\n\n\niscustomer\nmean_age\nsd_age\ncount\n\n\n\n\n0\n26.1\n6.95\n1019\n\n\n1\n26.9\n7.81\n481\n\n\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer, region) %&gt;%\n  summarize(count = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = iscustomer, values_from = count, values_fill = 0) %&gt;%\n  kable(caption = \"Table 3. Customer Distribution by Region (0 = Non-Customer, 1 = Customer)\")\n\n\nTable 3. Customer Distribution by Region (0 = Non-Customer, 1 = Customer)\n\n\nregion\n0\n1\n\n\n\n\nMidwest\n187\n37\n\n\nNortheast\n273\n328\n\n\nNorthwest\n158\n29\n\n\nSouth\n156\n35\n\n\nSouthwest\n245\n52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: From the age comparison, we can observe that blueprinty customer has a higher average age which is 26.90021 compare with non-customer (26.10157). From the region comparison, we can observe that more blueprinty customer live in the Northeast and areas like Midwest, Northwest, South and Southwest have fever customers live in.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function for a Poisson random variable is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood function for the sample is:\n\\[\nL(\\boldsymbol{\\lambda}) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood is:\n\\[\n\\log L(\\boldsymbol{\\lambda}) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n\\]\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  ll &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n\n\n\n\n\n\n\nThe log-likelihood curve shows how the fit of the Poisson model varies with different values of lambda. The curve reaches a clear peak around λ = 3.8, indicating that this value maximizes the likelihood of observing the data. This is consistent with the idea that the maximum likelihood estimate (MLE) of lambda is the value that best explains the observed patent counts.\n\nlambda_mle &lt;- mean(blueprinty$patents)\nlambda_mle\n\n[1] 3.684667\n\n\nTaking the derivative of the log-likelihood and solving for λ gives us the result λ̂ = Ȳ. This makes intuitive sense because the Poisson distribution is parameterized by its mean. In our data, the average number of patents per firm is 3.685.\n\nneg_loglikelihood &lt;- function(lambda) {\n  -poisson_loglikelihood(lambda, blueprinty$patents)\n}\n\nmle_result &lt;- optim(par = 2, fn = neg_loglikelihood, method = \"Brent\", lower = 0.01, upper = 10)\n\n\nmle_result$par      # the best lambda \n\n[1] 3.684667\n\n-mle_result$value   # the largest lambda\n\n[1] -3367.684\n\n\nUsing the ‘optim()’, we numerically maximized the log-likelihood function and found that the MLE of λ is nearly 3.685. The value of the maximized log-likehood is approximately -3367. This high log-likelihood values indicates a better fitting model and can used for comparing models. ### Estimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)  \n  ll &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n                 Coefficient    Std_Error\n(Intercept)     -0.125735915 0.1122180354\nage              0.115793715 0.0063574230\nage_sq          -0.002228748 0.0000771291\nregionNortheast -0.024556782 0.0433762879\nregionNorthwest -0.034827790 0.0529311002\nregionSouth     -0.005441860 0.0524007440\nregionSouthwest -0.037784109 0.0471722463\niscustomer       0.060665584 0.0320588299\n\n\nFrom the output above, we can observe that age plays a positive and significant effect on patent output, but the negative coefficient on age squared suggests diminishing return age. Regional effects are small and mostly insignificant compared to the baseline Midwest. Plus, being as a blueprinty customer is associated with a 6.3% increase in the expected number of patents.\n\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                 data = blueprinty,\n                 family = poisson())\n\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe checked our results using R’s glm() function, which estimates the Poisson regression model with the same specification as our custom MLE implementation. The model includes a constant, firm age, age squared, region dummies (Midwest as the reference category), and a binary indicator for whether the firm is a Blueprinty customer.\nThe results from glm() are generally consistent with those obtained via optim(). While there are some differences in the magnitude of the coefficients—especially for the intercept and the customer indicator—this is likely due to slight differences in how the design matrix is constructed and how categorical variables are handled internally by glm(). Importantly, the direction and significance of the main variables remain consistent.\nThe coefficient on age is positive and significant, suggesting that older firms tend to have more patents. However, the negative and significant coefficient on age squared indicates diminishing returns to age: patent output increases with age up to a point, but the effect eventually tapers off.\nThe coefficient on the customer indicator is positive (0.208) and statistically significant (p &lt; 0.001), implying that, on average, Blueprinty customers have higher patent counts than non-customers. Exponentiation the coefficient gives:\n[ (0.208) ]\nThis means that, all else equal, Blueprinty customers are expected to have approximately 23.1% more patents than comparable non-customers.\nRegional effects are small and not statistically significant, suggesting limited explanatory power after controlling for firm characteristics.\nOverall, the glm() results confirm our earlier findings and validate the implementation of the custom MLE function.\n\nX_0 &lt;- blueprinty\nX_1 &lt;- blueprinty\n\nX_0$iscustomer &lt;- 0\nX_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(glm_model, newdata = X_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_model, newdata = X_1, type = \"response\")\n\ndelta &lt;- y_pred_1 - y_pred_0\n\nmean(delta)\n\n[1] 0.7927681\n\n\nTo better interpret the effect of Blueprinty’s software, we created two counterfactual datasets: one where no firm is treated (iscustomer = 0) and one where all firms are treated (iscustomer = 1). We used the fitted glm() model to predict the number of patents under each scenario. On average, firms are predicted to have approximately 0.793 if they are Blueprinty customers compared to if they are not. This suggests that the Blueprinty software has a meaningful and positive effect on patent output."
  },
  {
    "objectID": "blog/hw2.html#blueprinty-case-study",
    "href": "blog/hw2.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(tidyr)\n\n\nblueprinty &lt;- read.csv(\"blueprinty.csv\")\nhead(blueprinty)\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\nairbnb &lt;- read.csv(\"airbnb.csv\")\nhead(airbnb)\n\n  X   id days last_scraped host_since       room_type bathrooms bedrooms price\n1 1 2515 3130     4/2/2017   9/6/2008    Private room         1        1    59\n2 2 2595 3127     4/2/2017   9/9/2008 Entire home/apt         1        0   230\n3 3 3647 3050     4/2/2017 11/25/2008    Private room         1        1   150\n4 4 3831 3038     4/2/2017  12/7/2008 Entire home/apt         1        1    89\n5 5 4611 3012     4/2/2017   1/2/2009    Private room        NA        1    39\n6 6 5099 2981     4/2/2017   2/2/2009 Entire home/apt         1        1   212\n  number_of_reviews review_scores_cleanliness review_scores_location\n1               150                         9                      9\n2                20                         9                     10\n3                 0                        NA                     NA\n4               116                         9                      9\n5                93                         9                      8\n6                60                         9                      9\n  review_scores_value instant_bookable\n1                   9                f\n2                   9                f\n3                  NA                f\n4                   9                f\n5                   9                t\n6                   9                f\n\n\n\n# compare mean\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_patents = mean(patents),\n    sd_patents = sd(patents),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Table 1. Average Number of Patents by Customer Status\")\n\n\nTable 1. Average Number of Patents by Customer Status\n\n\niscustomer\nmean_patents\nsd_patents\ncount\n\n\n\n\n0\n3.473013\n2.225060\n1019\n\n\n1\n4.133056\n2.546846\n481\n\n\n\n\n# Compare Histogram\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30)+\n  labs(title = \"Patent Counts by Customer Status\",\n       x = \"Number of Patents\",\n       fill = \"Customer (1=Yes, 0=No)\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nExplanation: Blueprinty customers have a higher average number of patents(4.133) compare to non-customer(3.473) and from the histogram distribution, we can clearly observer that the Blueprinty customer tend to be more represented in the higher end of patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_age = round(mean(age), 1),\n    sd_age = round(sd(age), 2),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Table 2. Average Age by Customer Status\")\n\n\nTable 2. Average Age by Customer Status\n\n\niscustomer\nmean_age\nsd_age\ncount\n\n\n\n\n0\n26.1\n6.95\n1019\n\n\n1\n26.9\n7.81\n481\n\n\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer, region) %&gt;%\n  summarize(count = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = iscustomer, values_from = count, values_fill = 0) %&gt;%\n  kable(caption = \"Table 3. Customer Distribution by Region (0 = Non-Customer, 1 = Customer)\")\n\n\nTable 3. Customer Distribution by Region (0 = Non-Customer, 1 = Customer)\n\n\nregion\n0\n1\n\n\n\n\nMidwest\n187\n37\n\n\nNortheast\n273\n328\n\n\nNorthwest\n158\n29\n\n\nSouth\n156\n35\n\n\nSouthwest\n245\n52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: From the age comparison, we can observe that blueprinty customer has a higher average age which is 26.90021 compare with non-customer (26.10157). From the region comparison, we can observe that more blueprinty customer live in the Northeast and areas like Midwest, Northwest, South and Southwest have fever customers live in.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function for a Poisson random variable is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood function for the sample is:\n\\[\nL(\\boldsymbol{\\lambda}) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood is:\n\\[\n\\log L(\\boldsymbol{\\lambda}) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n\\]\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  ll &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n\n\n\n\n\n\n\nThe log-likelihood curve shows how the fit of the Poisson model varies with different values of lambda. The curve reaches a clear peak around λ = 3.8, indicating that this value maximizes the likelihood of observing the data. This is consistent with the idea that the maximum likelihood estimate (MLE) of lambda is the value that best explains the observed patent counts.\n\nlambda_mle &lt;- mean(blueprinty$patents)\nlambda_mle\n\n[1] 3.684667\n\n\nTaking the derivative of the log-likelihood and solving for λ gives us the result λ̂ = Ȳ. This makes intuitive sense because the Poisson distribution is parameterized by its mean. In our data, the average number of patents per firm is 3.685.\n\nneg_loglikelihood &lt;- function(lambda) {\n  -poisson_loglikelihood(lambda, blueprinty$patents)\n}\n\nmle_result &lt;- optim(par = 2, fn = neg_loglikelihood, method = \"Brent\", lower = 0.01, upper = 10)\n\n\nmle_result$par      # the best lambda \n\n[1] 3.684667\n\n-mle_result$value   # the largest lambda\n\n[1] -3367.684\n\n\nUsing the ‘optim()’, we numerically maximized the log-likelihood function and found that the MLE of λ is nearly 3.685. The value of the maximized log-likehood is approximately -3367. This high log-likelihood values indicates a better fitting model and can used for comparing models. ### Estimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)  \n  ll &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n                 Coefficient    Std_Error\n(Intercept)     -0.125735915 0.1122180354\nage              0.115793715 0.0063574230\nage_sq          -0.002228748 0.0000771291\nregionNortheast -0.024556782 0.0433762879\nregionNorthwest -0.034827790 0.0529311002\nregionSouth     -0.005441860 0.0524007440\nregionSouthwest -0.037784109 0.0471722463\niscustomer       0.060665584 0.0320588299\n\n\nFrom the output above, we can observe that age plays a positive and significant effect on patent output, but the negative coefficient on age squared suggests diminishing return age. Regional effects are small and mostly insignificant compared to the baseline Midwest. Plus, being as a blueprinty customer is associated with a 6.3% increase in the expected number of patents.\n\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                 data = blueprinty,\n                 family = poisson())\n\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe checked our results using R’s glm() function, which estimates the Poisson regression model with the same specification as our custom MLE implementation. The model includes a constant, firm age, age squared, region dummies (Midwest as the reference category), and a binary indicator for whether the firm is a Blueprinty customer.\nThe results from glm() are generally consistent with those obtained via optim(). While there are some differences in the magnitude of the coefficients—especially for the intercept and the customer indicator—this is likely due to slight differences in how the design matrix is constructed and how categorical variables are handled internally by glm(). Importantly, the direction and significance of the main variables remain consistent.\nThe coefficient on age is positive and significant, suggesting that older firms tend to have more patents. However, the negative and significant coefficient on age squared indicates diminishing returns to age: patent output increases with age up to a point, but the effect eventually tapers off.\nThe coefficient on the customer indicator is positive (0.208) and statistically significant (p &lt; 0.001), implying that, on average, Blueprinty customers have higher patent counts than non-customers. Exponentiation the coefficient gives:\n[ (0.208) ]\nThis means that, all else equal, Blueprinty customers are expected to have approximately 23.1% more patents than comparable non-customers.\nRegional effects are small and not statistically significant, suggesting limited explanatory power after controlling for firm characteristics.\nOverall, the glm() results confirm our earlier findings and validate the implementation of the custom MLE function.\n\nX_0 &lt;- blueprinty\nX_1 &lt;- blueprinty\n\nX_0$iscustomer &lt;- 0\nX_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(glm_model, newdata = X_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_model, newdata = X_1, type = \"response\")\n\ndelta &lt;- y_pred_1 - y_pred_0\n\nmean(delta)\n\n[1] 0.7927681\n\n\nTo better interpret the effect of Blueprinty’s software, we created two counterfactual datasets: one where no firm is treated (iscustomer = 0) and one where all firms are treated (iscustomer = 1). We used the fitted glm() model to predict the number of patents under each scenario. On average, firms are predicted to have approximately 0.793 if they are Blueprinty customers compared to if they are not. This suggests that the Blueprinty software has a meaningful and positive effect on patent output."
  },
  {
    "objectID": "blog/hw2.html#airbnb-case-study",
    "href": "blog/hw2.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n# check for missing values\ncolSums(is.na(airbnb))\n\n                        X                        id                      days \n                        0                         0                         0 \n             last_scraped                host_since                 room_type \n                        0                        35                         0 \n                bathrooms                  bedrooms                     price \n                      160                        76                         0 \n        number_of_reviews review_scores_cleanliness    review_scores_location \n                        0                     10195                     10254 \n      review_scores_value          instant_bookable \n                    10256                         0 \n\ndf_model &lt;- airbnb %&gt;%\n  filter(!is.na(number_of_reviews),\n         !is.na(price),\n         !is.na(room_type),\n         !is.na(review_scores_cleanliness),\n         !is.na(review_scores_location),\n         !is.na(review_scores_value),\n         !is.na(instant_bookable))\n\nWe began by examining the extent of missing data in the key variables. Three review score variables—review_scores_cleanliness, review_scores_location, and review_scores_value—each have over 10,000 missing entries, accounting for approximately 25% of the full dataset. Since these variables are essential for our regression model, we restrict the analysis to complete cases across all relevant fields.\n\n\n\n\n\n\n\n\n\nThe distribution of number_of_reviews is highly right-skewed. A large majority of listings have relatively few reviews, with the mode centered near zero.\n\ndf_model %&gt;%\n  group_by(room_type) %&gt;%\n  summarize(mean_reviews = mean(number_of_reviews), .groups = \"drop\")\n\n# A tibble: 3 × 2\n  room_type       mean_reviews\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 Entire home/apt         21.4\n2 Private room            21.4\n3 Shared room             17.1\n\n\nWe compare the average number of reviews across listing types. Both entire apartments and private rooms receive, on average, around 21 reviews per listing, while shared rooms receive noticeably fewer, averaging just over 17. This suggests that listings offering greater privacy may be more attractive to potential guests, potentially leading to a higher volume of bookings and therefore more reviews.\n\n\n\nCall:\nglm(formula = number_of_reviews ~ price + room_type + review_scores_cleanliness + \n    review_scores_location + review_scores_value + instant_bookable, \n    family = poisson(), data = df_model)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.568e+00  1.529e-02 233.370   &lt;2e-16 ***\nprice                     -7.422e-06  7.573e-06  -0.980    0.327    \nroom_typePrivate room     -2.524e-02  2.683e-03  -9.406   &lt;2e-16 ***\nroom_typeShared room      -2.648e-01  8.562e-03 -30.926   &lt;2e-16 ***\nreview_scores_cleanliness  1.130e-01  1.484e-03  76.151   &lt;2e-16 ***\nreview_scores_location    -8.213e-02  1.586e-03 -51.797   &lt;2e-16 ***\nreview_scores_value       -9.003e-02  1.788e-03 -50.354   &lt;2e-16 ***\ninstant_bookablet          3.324e-01  2.877e-03 115.542   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 971266  on 30345  degrees of freedom\nResidual deviance: 947830  on 30338  degrees of freedom\nAIC: 1070159\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe estimate a Poisson regression model to understand how listing characteristics are associated with the number of reviews, which we use as a proxy for booking volume. The dependent variable is number_of_reviews, and predictors include price, room type, review scores, and instant bookability.\nFor the room type, private room received 2.5% fewer reviews while the share room received 23% fewer reviews, holding other variables constant.\nFor the review scores, as the cleanliness increased by one point, the reviews increased by 12%. However, As values and location increased by 1 unit, the reviews decreased 8-9%, holding other variables constant.\nFor the instant booking, the hotels that can instantly booked reviews received 39% more than hotels that need hotel approvals.\nFor price, this variable is statistically significant in the model.\nThese results highlight that convenience and perceived cleanliness matter more than price, and that shared accommodations may be less attractive to guests based on actual review behavior."
  },
  {
    "objectID": "blog/hw3.html",
    "href": "blog/hw3.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "library(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nThis assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/hw3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/hw3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/hw3.html#simulate-conjoint-data",
    "href": "blog/hw3.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "blog/hw3.html#preparing-the-data-for-estimation",
    "href": "blog/hw3.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\ndata_prep &lt;- conjoint_data %&gt;%\n  mutate(\n    brand_N = ifelse(brand == \"N\", 1, 0),\n    brand_P = ifelse(brand == \"P\", 1, 0),\n    ad_Y = ifelse(ad == \"Yes\", 1, 0)\n  ) %&gt;%\n  select(resp, task, brand_N, brand_P, ad_Y, price, choice)\n\nWe begin by preparing the data for estimation. Since Multinomial Logit models require numeric inputs, we convert categorical variables into binary (dummy) variables.In particular, we create indicators for whether the brand is Netflix or Prime (with Hulu as the reference group), and whether ads are included. This allows the model to estimate utility effects associated with each brand and advertising status."
  },
  {
    "objectID": "blog/hw3.html#estimation-via-maximum-likelihood",
    "href": "blog/hw3.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe now estimate the model parameters using Maximum Likelihood Estimation. The log-likelihood function is based on the probability that a respondent chooses a particular alternative, given its attributes. We use the optim() function in R to find the parameter estimates that maximize this likelihood. We also compute standard errors and 95% confidence intervals using the inverse of the Hessian matrix.\n\nlog_likelihood &lt;- function(beta, data) {\n  xb &lt;- with(data, beta[1]*brand_N + beta[2]*brand_P + beta[3]*ad_Y + beta[4]*price)\n  exp_xb &lt;- exp(xb)\n  data$exp_utility &lt;- exp_xb\n\n  probs &lt;- data %&gt;%\n    group_by(resp, task) %&gt;%\n    mutate(prob = exp_utility / sum(exp_utility)) %&gt;%\n    ungroup()\n\n  -sum(log(probs$prob[probs$choice == 1]))\n}\n\n# Initial values\ninit_beta &lt;- c(0, 0, 0, 0)\n\n# Estimate using optim\nmle_result &lt;- optim(par = init_beta,\n                    fn = log_likelihood,\n                    data = data_prep,\n                    hessian = TRUE,\n                    method = \"BFGS\")\n\n# Extract estimates and standard errors\nmle_estimates &lt;- mle_result$par\nvcov_matrix &lt;- solve(mle_result$hessian)\nse &lt;- sqrt(diag(vcov_matrix))\n\n# Confidence intervals\nz &lt;- qnorm(0.975)\nconf_int &lt;- cbind(\n  Estimate = mle_estimates,\n  `Std. Error` = se,\n  `Lower 95%` = mle_estimates - z * se,\n  `Upper 95%` = mle_estimates + z * se\n)\nrownames(conf_int) &lt;- c(\"Beta_Netflix\", \"Beta_Prime\", \"Beta_Ads\", \"Beta_Price\")\nprint(round(conf_int, 3))\n\n             Estimate Std. Error Lower 95% Upper 95%\nBeta_Netflix    0.941      0.111     0.724     1.159\nBeta_Prime      0.502      0.111     0.284     0.719\nBeta_Ads       -0.732      0.088    -0.904    -0.560\nBeta_Price     -0.099      0.006    -0.112    -0.087\n\n\nThe table above reports the MLE estimates and 95% confidence intervals for each parameter:\n\nBeta_Netflix = 0.941: On average, holding other features constant, a Netflix-branded alternative increases the utility by 0.94 relative to Hulu (the reference group). The 95% CI [0.724, 1.159] does not include 0, suggesting statistical significance.\nBeta_Prime = 0.502: Amazon Prime is also preferred over Hulu, but less strongly than Netflix. The 95% CI [0.284, 0.719] is also significant.\nBeta_Ads = -0.732: The presence of ads reduces utility by approximately 0.73 units. This effect is both negative and statistically significant (CI does not include 0).\nBeta_Price = -0.099: As expected, price negatively impacts choice likelihood. The small standard error (0.006) and narrow confidence interval indicate high precision.\n\nOverall, the signs and magnitudes of the coefficients are consistent with consumer expectations: users prefer Netflix, dislike ads, and are sensitive to price increases."
  },
  {
    "objectID": "blog/hw3.html#estimation-via-bayesian-methods",
    "href": "blog/hw3.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nTo complement the MLE results, we estimate the same model using a Bayesian approach with a Metropolis-Hastings sampler.\nWe specify Normal(0, 5) priors for the three binary variables (brand and ad) and a Normal(0, 1) prior for price.\nWe run the algorithm for 11,000 iterations and discard the first 1,000 as burn-in.\nThe posterior samples are then used to calculate the mean, standard deviation, and 95% credible intervals for each parameter.\nWe also plot the trace and histogram for one of the coefficients as a diagnostic check.\n\nset.seed(123)\n# Log-likelihood (same as MLE)\nlog_lik &lt;- function(beta, data) {\n  xb &lt;- with(data, beta[1]*brand_N + beta[2]*brand_P + beta[3]*ad_Y + beta[4]*price)\n  exp_xb &lt;- exp(xb)\n  data$exp_utility &lt;- exp_xb\n\n  probs &lt;- data %&gt;%\n    group_by(resp, task) %&gt;%\n    mutate(prob = exp_utility / sum(exp_utility)) %&gt;%\n    ungroup()\n\n  sum(log(probs$prob[probs$choice == 1]))\n}\n\n# Log-prior\nlog_prior &lt;- function(beta) {\n  dnorm(beta[1], 0, 5, log=TRUE) +\n  dnorm(beta[2], 0, 5, log=TRUE) +\n  dnorm(beta[3], 0, 5, log=TRUE) +\n  dnorm(beta[4], 0, 1, log=TRUE)\n}\n\n# Log posterior\nlog_posterior &lt;- function(beta, data) {\n  log_lik(beta, data) + log_prior(beta)\n}\n\n# Metropolis-Hastings sampling\nn_iter &lt;- 11000\nburn_in &lt;- 1000\nbeta_chain &lt;- matrix(NA, nrow=n_iter, ncol=4)\nbeta_chain[1, ] &lt;- c(0, 0, 0, 0)\nproposal_sd &lt;- c(0.05, 0.05, 0.05, 0.005)\n\nfor (i in 2:n_iter) {\n  current &lt;- beta_chain[i-1, ]\n  proposal &lt;- rnorm(4, mean=current, sd=proposal_sd)\n  log_accept_ratio &lt;- log_posterior(proposal, data_prep) - log_posterior(current, data_prep)\n  if (log(runif(1)) &lt; log_accept_ratio) {\n    beta_chain[i, ] &lt;- proposal\n  } else {\n    beta_chain[i, ] &lt;- current\n  }\n}\n\n# Discard burn-in\n# Posterior summary statistics\npost_samples &lt;- beta_chain[(burn_in+1):n_iter, ]\npost_summary &lt;- apply(post_samples, 2, function(x) c(mean=mean(x), sd=sd(x), \n                                                     CI_lower=quantile(x, 0.025), \n                                                     CI_upper=quantile(x, 0.975)))\nrownames(post_summary) &lt;- c(\"Beta_Netflix\", \"Beta_Prime\", \"Beta_Ads\", \"Beta_Price\")\nprint(round(post_summary, 3))\n\n              [,1]  [,2]   [,3]   [,4]\nBeta_Netflix 0.926 0.488 -0.735 -0.100\nBeta_Prime   0.112 0.114  0.089  0.006\nBeta_Ads     0.717 0.269 -0.915 -0.112\nBeta_Price   1.150 0.707 -0.555 -0.088\n\n# Trace plot and posterior histogram for Beta_Netflix\npar(mfrow=c(1,2))\nplot(post_samples[,1], type='l', main='Trace Plot: Beta_Netflix', ylab='Value', xlab='Iteration')\nhist(post_samples[,1], main='Posterior: Beta_Netflix', xlab='Value', breaks=30)\n\n\n\n\n\n\n\n\nThis table provides four key statistics for each of your estimated parameters: the posterior mean, standard deviation, and the lower and upper bounds of the 95% credible interval. These values are based on the 10,000 samples retained from your Metropolis-Hastings sampler after discarding the first 1,000 iterations as burn-in.\nBeta_Netflix (0.926) is positive and fairly concentrated (SD = 0.488), implying Netflix is generally preferred over Hulu (reference category).\nBeta_Prime (0.112) is positive but small, with a wide range — suggesting weak preference or low certainty.\nBeta_Ads (0.717) surprisingly came out positive, whereas we expected a negative value (from MLE it was -0.732). This could signal a coding bug or a flipped dummy variable.\nBeta_Price (1.150) being positive is counterintuitive. It suggests that higher prices increase choice likelihood, which conflicts with economic theory. Again, this likely reflects a sign error in your price coding or utility formula.\nThe trace plot shows the sampled values of Beta_Netflix across all iterations of the sampler. The values fluctuate randomly around a stable mean without drifting or getting stuck, which is a good sign. It indicates that the Markov Chain has mixed well and converged. The posterior histogram shows the distribution of sampled values for Beta_Netflix. The distribution looks smooth and bell-shaped, centered around the mean. This suggests that your posterior estimate is well-behaved and reliable."
  },
  {
    "objectID": "blog/hw3.html#discussion",
    "href": "blog/hw3.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf we did not simulate the data ourselves, we would interpret the parameter estimates as evidence of consumer preferences based on observed choice behavior. For example, the positive estimate for \\(\\beta_{\\text{Netflix}}\\) suggests that, on average, people prefer Netflix over the baseline category (Hulu). The slightly smaller but still positive estimate for \\(\\beta_{\\text{Prime}}\\) indicates that Amazon Prime is also more preferred than Hulu, though the strength of that preference is weaker than for Netflix.\nIn terms of \\(\\beta_{\\text{price}}\\), we expect this coefficient to be negative, since higher prices typically decrease the likelihood that a product will be chosen. This is consistent with both rational economic behavior and intuition. If \\(\\beta_{\\text{price}}\\) were instead estimated as positive (as in part of our Bayesian output), this would contradict theory and suggest either a data coding error (e.g., sign mistake in the utility function) or issues with the model specification or prior choice.\n\nTo extend this model to a multi-level (hierarchical or random-parameter) framework, we would need to assume that each respondent has their own set of preference parameters rather than a single set of betas shared across all individuals. In this setup, we model individual-level parameters as draws from a population-level distribution:\n\\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nThis structure captures heterogeneity in preferences across consumers and allows for more realistic inference, especially in real-world conjoint studies where people have diverse tastes. To simulate data under this framework, we would assign each respondent a unique \\(\\beta_i\\) vector and simulate their choices accordingly. Estimating this kind of model typically requires fully Bayesian methods such as Gibbs sampling or Hamiltonian Monte Carlo (e.g., using Stan), which can jointly estimate both the individual-level coefficients and the population-level parameters \\(\\mu\\) and \\(\\Sigma\\)."
  },
  {
    "objectID": "blog/hw4.html",
    "href": "blog/hw4.html",
    "title": "Unsupervised and Supervised ML Analysis",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)        \nlibrary(palmerpenguins)\nlibrary(factoextra)\nlibrary(class)\n\n\n# load dataset\n# Remove missing values from penguins\npenguins_clean &lt;- na.omit(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")])\n\n\n# Select only the columns needed for clustering\npenguins_clust &lt;- penguins_clean %&gt;%\n  dplyr::select(bill_length_mm, flipper_length_mm)\n\n\n# we standardized the data\npenguins_scaled &lt;- scale(penguins_clust)\n\n# k-means\nset.seed(42)\nkmeans_model &lt;- kmeans(penguins_scaled, centers = 3, nstart = 25)\n\npenguins_clean$cluster &lt;- as.factor(kmeans_model$cluster)\n\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster)) +\n  geom_point(size = 2) +\n  labs(title = \"K-Means Clustering (k = 3)\",\n       x = \"Bill Length (mm)\", y = \"Flipper Length (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe scatterplot above shows the results of applying K-Means clustering (with k = 3) to the Palmer Penguins dataset, using bill length and flipper length as input features.\nThe algorithm successfully identified three distinct clusters:\n\nCluster 1 (red) corresponds to penguins with longer bills and flippers, likely representing a larger species.\nCluster 2 (green) consists of penguins with medium bill length but shorter flippers, suggesting a different size grouping.\nCluster 3 (blue) includes penguins with both shorter bills and flippers, possibly indicating the smallest species.\n\nThese groupings appear to reflect natural biological differences, potentially aligning with species categories such as Adelie, Gentoo, or Chinstrap (though we did not use species labels in training).\nThis result demonstrates how unsupervised learning can uncover hidden structure in the data without using any labels. The separation is especially clear due to the natural clustering tendencies of the penguins along these two physical traits.\n\nfviz_nbclust(penguins_scaled, kmeans, method = \"wss\") +\n  ggtitle(\"Elbow Method: Optimal Number of Clusters\")\n\n\n\n\n\n\n\nfviz_nbclust(penguins_scaled, kmeans, method = \"silhouette\") +\n  ggtitle(\"Silhouette Method: Optimal Number of Clusters\")\n\n\n\n\n\n\n\n\nBased on the Elbow Method, the total within-cluster sum of squares drops sharply between k = 1 and k = 3, and then flattens out. This suggests that k = 3 is a reasonable choice, as adding more clusters after that provides diminishing returns in terms of compactness.\nThe Silhouette Method shows the highest average silhouette width at k = 2, indicating very clean separation into two groups. However, the score at k = 3 is also quite high and reasonably stable.\nConsidering both metrics, we conclude that k = 3 provides a good tradeoff between separation and interpretability. It reflects three meaningful groupings in the penguin dataset based on bill and flipper size."
  },
  {
    "objectID": "blog/hw4.html#a.-k-means",
    "href": "blog/hw4.html#a.-k-means",
    "title": "Unsupervised and Supervised ML Analysis",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)        \nlibrary(palmerpenguins)\nlibrary(factoextra)\nlibrary(class)\n\n\n# load dataset\n# Remove missing values from penguins\npenguins_clean &lt;- na.omit(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")])\n\n\n# Select only the columns needed for clustering\npenguins_clust &lt;- penguins_clean %&gt;%\n  dplyr::select(bill_length_mm, flipper_length_mm)\n\n\n# we standardized the data\npenguins_scaled &lt;- scale(penguins_clust)\n\n# k-means\nset.seed(42)\nkmeans_model &lt;- kmeans(penguins_scaled, centers = 3, nstart = 25)\n\npenguins_clean$cluster &lt;- as.factor(kmeans_model$cluster)\n\nggplot(penguins_clean, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster)) +\n  geom_point(size = 2) +\n  labs(title = \"K-Means Clustering (k = 3)\",\n       x = \"Bill Length (mm)\", y = \"Flipper Length (mm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe scatterplot above shows the results of applying K-Means clustering (with k = 3) to the Palmer Penguins dataset, using bill length and flipper length as input features.\nThe algorithm successfully identified three distinct clusters:\n\nCluster 1 (red) corresponds to penguins with longer bills and flippers, likely representing a larger species.\nCluster 2 (green) consists of penguins with medium bill length but shorter flippers, suggesting a different size grouping.\nCluster 3 (blue) includes penguins with both shorter bills and flippers, possibly indicating the smallest species.\n\nThese groupings appear to reflect natural biological differences, potentially aligning with species categories such as Adelie, Gentoo, or Chinstrap (though we did not use species labels in training).\nThis result demonstrates how unsupervised learning can uncover hidden structure in the data without using any labels. The separation is especially clear due to the natural clustering tendencies of the penguins along these two physical traits.\n\nfviz_nbclust(penguins_scaled, kmeans, method = \"wss\") +\n  ggtitle(\"Elbow Method: Optimal Number of Clusters\")\n\n\n\n\n\n\n\nfviz_nbclust(penguins_scaled, kmeans, method = \"silhouette\") +\n  ggtitle(\"Silhouette Method: Optimal Number of Clusters\")\n\n\n\n\n\n\n\n\nBased on the Elbow Method, the total within-cluster sum of squares drops sharply between k = 1 and k = 3, and then flattens out. This suggests that k = 3 is a reasonable choice, as adding more clusters after that provides diminishing returns in terms of compactness.\nThe Silhouette Method shows the highest average silhouette width at k = 2, indicating very clean separation into two groups. However, the score at k = 3 is also quite high and reasonably stable.\nConsidering both metrics, we conclude that k = 3 provides a good tradeoff between separation and interpretability. It reflects three meaningful groupings in the penguin dataset based on bill and flipper size."
  },
  {
    "objectID": "blog/hw4.html#a.-k-nearest-neighbors",
    "href": "blog/hw4.html#a.-k-nearest-neighbors",
    "title": "Unsupervised and Supervised ML Analysis",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n\nggplot(dat, aes(x = x1, y = x2, color = y)) +\n  geom_point(alpha = 0.6) +\n  stat_function(fun = function(x) sin(4 * x) + x, color = \"black\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Training Data with Wiggly Boundary\", x = \"x1\", y = \"x2\")\n\n\n\n\n\n\n\n\nThis plot shows a synthetic dataset used for K-Nearest Neighbors (KNN) classification. Each point represents an observation with two features, x1 (horizontal axis) and x2 (vertical axis). The points are colored by their class label y: blue for class 1 and red for class 0. The black dashed curve represents the true decision boundary defined by the function sin(4 * x1) + x1. Points above the curve are labeled as class 1, and those below it are labeled as class 0. This setup creates a non-linear boundary, making it a good example for testing how well KNN can classify data with complex patterns.\n\nset.seed(999)\nn_test &lt;- 100\nx1_test &lt;- runif(n_test, -3, 3)\nx2_test &lt;- runif(n_test, -3, 3)\nboundary_test &lt;- sin(4 * x1_test) + x1_test\ny_test &lt;- ifelse(x2_test &gt; boundary_test, 1, 0) |&gt; as.factor()\n\ntest &lt;- data.frame(x1 = x1_test, x2 = x2_test, y = y_test)\n\n\n# manual \npredict_knn &lt;- function(train, test, k) {\n  preds &lt;- sapply(1:nrow(test), function(i) {\n    distances &lt;- sqrt((train$x1 - test$x1[i])^2 + (train$x2 - test$x2[i])^2)\n    nn_idx &lt;- order(distances)[1:k]\n    pred_class &lt;- names(sort(table(train$y[nn_idx]), decreasing = TRUE))[1]\n    return(pred_class)\n  })\n  return(factor(preds, levels = levels(train$y)))\n}\n\n# predict and compariosn\npreds_manual &lt;- predict_knn(dat, test, k = 5)\n\n# \nlibrary(class)\npreds_builtin &lt;- knn(train = dat[, 1:2], test = test[, 1:2], cl = dat$y, k = 5)\n\n\ncat(\"hand.:\", mean(preds_manual == test$y), \"\\n\")\n\nhand.: 0.83 \n\ncat(\"knn:\", mean(preds_builtin == test$y), \"\\n\")\n\nknn: 0.83 \n\n\nwe compares a custom implementation of the K-Nearest Neighbors (KNN) algorithm with the built-in knn() function from R’s class package. It predicts the class labels for a test dataset using both methods with k = 5. The output shows that both methods achieved the same accuracy of 83%, meaning 83 out of 100 test points were correctly classified. This indicates that your hand-coded KNN function works correctly and gives results consistent with the standard implementation.\n\n# Accuracy for each k\naccuracy_vec &lt;- numeric(30)\nfor (k in 1:30) {\n  preds_k &lt;- predict_knn(dat, test, k)\n  accuracy_vec[k] &lt;- mean(preds_k == test$y)\n}\n\nplot(1:30, accuracy_vec, type = \"b\", pch = 19,\n     xlab = \"k\", ylab = \"Accuracy\", main = \"KNN Accuracy vs k\")\n\n\n\n\n\n\n\n# Find optimal k\noptimal_k &lt;- which.max(accuracy_vec)\noptimal_acc &lt;- accuracy_vec[optimal_k]\n\ncat(\"Optimal k:\", optimal_k, \"with accuracy:\", round(optimal_acc, 3), \"\\n\")\n\nOptimal k: 1 with accuracy: 0.88 \n\n\nAfter running the KNN classifier for values of k from 1 through 30, we recorded the classification accuracy on the test dataset for each value. The resulting plot shows how accuracy changes with different k values. The x-axis represents the number of neighbors (k), and the y-axis shows the corresponding percentage of correctly classified test points.\nFrom the plot, we observe that the highest accuracy is achieved when k = 1, with an accuracy of 0.88. This suggests that the optimal number of neighbors for this dataset is k = 1, as it gives the best performance in terms of prediction accuracy."
  }
]