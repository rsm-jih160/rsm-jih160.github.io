[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiayi He",
    "section": "",
    "text": "Here is the summary of me!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "My Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/hw1.html",
    "href": "blog/hw1.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nDescription of the experiment: The experiment was designed to test whether offering a matching grant affects charitable giving. Karlan and List collaborated with a liberal nonprofit organization and sent fundraising letters to over 50,000 prior donors. These letters were randomly assigned to either a control group or a treatment group.\nThe control group received a standard fundraising letter. The treatment group received a nearly identical letter, except it included an announcement that a “concerned member” would match their donation. Within the treatment group, there were further randomizations: donors received different match ratios (1:1, 2:1, or 3:1), different maximum matching amounts ($25,000, $50,000, $100,000, or unspecified), and different suggested donation amounts (equal to, 1.25×, or 1.5× their previous highest contribution).\nThis design allows the authors to estimate both the overall effect of matching gifts and to test whether larger match ratios or different framing elements lead to higher donations. This project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1.html#introduction",
    "href": "blog/hw1.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nDescription of the experiment: The experiment was designed to test whether offering a matching grant affects charitable giving. Karlan and List collaborated with a liberal nonprofit organization and sent fundraising letters to over 50,000 prior donors. These letters were randomly assigned to either a control group or a treatment group.\nThe control group received a standard fundraising letter. The treatment group received a nearly identical letter, except it included an announcement that a “concerned member” would match their donation. Within the treatment group, there were further randomizations: donors received different match ratios (1:1, 2:1, or 3:1), different maximum matching amounts ($25,000, $50,000, $100,000, or unspecified), and different suggested donation amounts (equal to, 1.25×, or 1.5× their previous highest contribution).\nThis design allows the authors to estimate both the overall effect of matching gifts and to test whether larger match ratios or different framing elements lead to higher donations. This project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1.html#data",
    "href": "blog/hw1.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset contains 50,083 observations and 51 variables. Each observation corresponds to one prior donor who received a fundraising letter. Individuals were randomly assigned to a control group or a treatment group. Within the treatment group, they were further randomly assigned to different match ratios, match thresholds, and suggested donation amounts. Key variables include: - treatment: indicates if the donor received a matching gift offer (1 = yes, 0 = no) - ratio2, ratio3: dummies for match ratios of 2:1 and 3:1 (baseline is 1:1) - size25, size50, size100: dummies for match maximum amount - askd1, askd2, askd3: suggested donation amounts based on prior donation - amount: actual amount donated - gave: binary variable indicating whether the donor gave (1 = yes, 0 = no)\nWe will use these variables to replicate the results in Karlan and List (2007).\n\nlibrary(haven)\nlibrary(margins)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(knitr)\n\n\n\n\n\n\n\n\ndata &lt;- read_dta(\"karlan_list_2007.dta\")\n\n\nVariable Definitions\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n#month since last donation \ntidy(t.test(mrm2 ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  -0.0137      13.0      13.0    -0.120   0.905    33394.   -0.238     0.211\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(mrm2 ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  13.0       0.0935   139.      0    \n2 treatment     0.0137    0.115      0.119   0.905\n\n# gender \ntidy(t.test(female ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  0.00755     0.283     0.275      1.75  0.0795    32451. -0.000889    0.0160\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(female ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.283     0.00350     80.7   0     \n2 treatment   -0.00755   0.00429     -1.76  0.0787\n\n# couple\ntidy(t.test(couple ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  0.00162    0.0930    0.0914     0.582   0.560    32439. -0.00383   0.00706\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(couple ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  0.0930    0.00226    41.1     0    \n2 treatment   -0.00162   0.00277    -0.584   0.559\n\n\n\n\nBalance Test Results\nTo evaluate whether the randomization successfully created comparable groups, we examine balance on several pre-treatment covariates: months since last donation (mrm2), gender (female), and couple status.\n\nFor months since last donation, the mean in the control group is 12.998 months and 13.012 months in the treatment group. The t-test yields a p-value of 0.9049, and the linear regression coefficient is not significant. This suggests no systematic difference between groups.\nFor gender, the control group has a female proportion of 28.3%, compared to 27.5% in the treatment group. The t-test gives a p-value of 0.0795. Although this is closer to significance, it is still above the 5% threshold, and the regression confirms no strong evidence of imbalance.\nFor couple status, the proportions are 9.30% (control) and 9.14% (treatment), with a p-value of 0.5604. No evidence suggests imbalance on this variable either.\n\nThese results confirm that the treatment assignment appears balanced across key demographic characteristics, supporting the validity of the experimental design."
  },
  {
    "objectID": "blog/hw1.html#experimental-results",
    "href": "blog/hw1.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n# Calculate donation rate by treatment group\ndonation_rate &lt;- data %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(response_rate = mean(gave == 1, na.rm = TRUE))\n\n# Rename group labels\ndonation_rate$treatment &lt;- factor(donation_rate$treatment, labels = c(\"Control\", \"Treatment\"))\n\n# Plot\nggplot(donation_rate, aes(x = treatment, y = response_rate)) +\n  geom_col(fill = \"steelblue\", width = 0.6) +\n  geom_text(aes(label = scales::percent(response_rate, accuracy = 0.1)), vjust = -0.5) +\n  ylim(0, max(donation_rate$response_rate) + 0.05) +\n  labs(\n    title = \"Proportion of Donors by Treatment Group\",\n    x = \"Group\",\n    y = \"Proportion Who Donated\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe bar plots display the donation response rate. For the control group, we have 1.8% of individual donated and for the treatment group, we have 2.2% of individuals donated. This suggests that individuals who received the matching donation offer were more likely to donate compared to those who did not.\n\ntidy(t.test(gave ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 -0.00418    0.0179    0.0220     -3.21 0.00133    36577. -0.00673  -0.00163\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(gave ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.0179    0.00110     16.2  4.78e-59\n2 treatment    0.00418   0.00135      3.10 1.93e- 3\n\n\nThe results show a statistically significant difference: individuals in the treatment group were more likely to donate than those in the control group. While the donation rates were low overall — roughly 1.8% in the control group versus 2.2% in the treatment group — the difference of about 0.4 percentage points is small but meaningful, and it is unlikely to have occurred by chance (p-value &lt; 0.01). This tells us something powerful about human behavior: even a subtle nudge, like telling people their donation will be matched, can influence their decision to give. People seem more willing to act charitably when they believe their contribution will have a greater impact. In this case, the match offer acted as a psychological motivator, reinforcing the value and urgency of giving.\n\ndata &lt;- data %&gt;%\n  mutate(treatment = ifelse(ratio &gt; 0, 1, 0))\n\n# Probit model\nprobit_model &lt;- glm(gave ~ treatment, family = binomial(link = \"probit\"), data = data)\n\ntidy(probit_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -2.10      0.0233    -90.1  0      \n2 treatment     0.0868    0.0279      3.11 0.00185\n\n\n\nmfx &lt;- margins(probit_model)\nsummary(mfx)\n\n    factor    AME     SE      z      p  lower  upper\n treatment 0.0043 0.0014 3.1044 0.0019 0.0016 0.0070\n\n\nThe estimated average marginal effect of the treatment assignment is approximately 0.0043, meaning that the presence of a matching offer increased the probability of donating by about 0.4 percentage points.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# Filter treatment groups only\nmatch_data &lt;- data %&gt;%\n  filter(ratio %in% c(1, 2, 3)) %&gt;%\n  mutate(gave = ifelse(amount &gt; 0, 1, 0))\n\n# Split into groups\ngroup_1_1 &lt;- match_data %&gt;% filter(ratio == 1)\ngroup_2_1 &lt;- match_data %&gt;% filter(ratio == 2)\ngroup_3_1 &lt;- match_data %&gt;% filter(ratio == 3)\n\n# Run t-tests\nt_2_vs_1 &lt;- tidy(t.test(group_2_1$gave, group_1_1$gave))\nt_3_vs_1 &lt;- tidy(t.test(group_3_1$gave, group_1_1$gave))\n\nt_2_vs_1\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  0.00188    0.0226    0.0207     0.965   0.335    22225. -0.00194   0.00571\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\nt_3_vs_1\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1  0.00198    0.0227    0.0207      1.02   0.310    22215. -0.00185   0.00582\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nNeither the $2:$1 nor the $3:$1 match rate significantly increases the probability of donation compared to the $1:$1 rate. This aligns with the authors’ interpretation on page 8 — that larger match ratios do not provide additional effectiveness. Our t-tests support this claim: p-values for both comparisons are well above conventional thresholds for statistical significance.\n\n# Use ratio1 as the base category\nmatch_data &lt;- match_data %&gt;%\n  mutate(\n    ratio2 = as.numeric(ratio == 2),\n    ratio3 = as.numeric(ratio == 3)\n  )\n\n# Regression using ratio2 and ratio3 (1:1 is base)\nmodel &lt;- glm(gave ~ ratio2 + ratio3, family = binomial(link = \"probit\"), data = match_data)\n\n# Show marginal effects\ntidy(margins(model))\n\n# A tibble: 2 × 5\n  term   estimate std.error statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 ratio2  0.00191   0.00198     0.965   0.335\n2 ratio3  0.00201   0.00198     1.01    0.310\n\n\nThe estimated marginal effect for the 2:1 match is 0.0019 with a p-value of 0.335. The estimated marginal effect for the 3:1 match is 0.0020 with a p-value of 0.310. These results are not statistically significant, indicating that neither the 2:1 nor the 3:1 match leads to a higher probability of giving compared to the 1:1 match.\n\n# Direct from data\nresponse_rates &lt;- match_data %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(response_rate = mean(gave))\n\nrate_2_vs_1 &lt;- response_rates$response_rate[response_rates$ratio == 2] - \n               response_rates$response_rate[response_rates$ratio == 1]\n\nrate_3_vs_2 &lt;- response_rates$response_rate[response_rates$ratio == 3] - \n               response_rates$response_rate[response_rates$ratio == 2]\n\n# Run the probit model using ratio as a categorical variable\nmodel_factor &lt;- glm(gave ~ factor(ratio), family = binomial(link = \"probit\"), data = match_data)\n\nmfx &lt;- summary(margins(model_factor))\n\nmfx_diff_2_vs_1 &lt;- mfx$AME[mfx$factor == \"ratio2\"]\nmfx_diff_3_vs_2 &lt;- mfx$AME[mfx$factor == \"ratio3\"] - mfx$AME[mfx$factor == \"ratio2\"]\n\nlist(\n  direct_data_differences = list(\n    `2:1 vs 1:1` = rate_2_vs_1,\n    `3:1 vs 2:1` = rate_3_vs_2\n  ),\n  regression_marginal_effect_differences = list(\n    `2:1 vs 1:1` = mfx_diff_2_vs_1,\n    `3:1 vs 2:1` = mfx_diff_3_vs_2\n  )\n)\n\n$direct_data_differences\n$direct_data_differences$`2:1 vs 1:1`\n[1] 0.001884251\n\n$direct_data_differences$`3:1 vs 2:1`\n[1] 0.000100024\n\n\n$regression_marginal_effect_differences\n$regression_marginal_effect_differences$`2:1 vs 1:1`\n     ratio2 \n0.001884251 \n\n$regression_marginal_effect_differences$`3:1 vs 2:1`\n     ratio3 \n0.000100024 \n\n\nFrom both the raw data and the regression model, we find: - The difference in response rate between 2:1 and 1:1 is approximately 0.00188 (0.19 percentage points). - The difference in response rate between 3:1 and 2:1 is a mere 0.00010 (0.01 percentage points).\nThese values are extremely small, and consistent across both the direct calculation from raw data and the marginal effects estimated from the probit model. The response rate differences between 1:1, 2:1, and 3:1 match ratios are economically negligible and statistically insignificant. This strongly supports the conclusion drawn in Karlan and List (2007):\n&gt; “Larger match ratios—$3:$1 and $2:$1—relative to smaller match ratios ($1:$1) had no additional impact.” Increasing the match ratio beyond 1:1 does not improve donor response rates and may not be a cost-effective strategy for fundraisers.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\ntidy(t.test(amount ~ treatment, data = data))\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1   -0.154     0.813     0.967     -1.92  0.0551    36216.   -0.311   0.00334\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\ntidy(lm(amount ~ treatment, data = data))\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.813    0.0674     12.1  1.84e-33\n2 treatment      0.154    0.0826      1.86 6.28e- 2\n\n\nThe result from t-test, we have mean in control group is 0.81 and treatment group is 0.97, p-value is 0.055. The avg donation amount is slightly higher in the treatment group means that matching grants may slightly increase donation amounts but the main thing is to give people the incentive to give not on how much thet give.\n\n# Filter for donors only\ndonors_only &lt;- data %&gt;%\n  filter(amount &gt; 0)\n\n# Run OLS regression on positive donations\nmodel_conditional &lt;- lm(amount ~ treatment, data = donors_only)\n\ntidy(model_conditional)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    45.5       2.42    18.8   5.47e-68\n2 treatment      -1.67      2.87    -0.581 5.61e- 1\n\n\nFrom the output, the intercepts reflects the average donation amount in the control group. And the coefficient on treatment shows the average increase (or decrease) in donation amount among donors. This coefficient does not have a clear causal interpretation since we are limited the condition to gave =1 and the selection bias exists since the treatment may impact people who donates in the first place. Thus, this regression cannot identify the causal effect of treatment on donation size.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "blog/hw1.html#simulation-experiment",
    "href": "blog/hw1.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\n\nThe simulation illustrates the Law of Large Numbers in action. Initially, the cumulative average of the simulated differences is noisy and jumps around, but as the number of draws increases, it converges steadily toward the true mean difference of 0.004.This demonstrates how, in repeated sampling, the sample average becomes a reliable estimator of the population average — the foundational idea behind statistical inference methods like the t-test used earlier in this blog.It reinforces why even small effects (like a 0.004 difference in donation likelihood) can be detected reliably with large samples.\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\nThis simulation illustrates the Central Limit Theorem using differences in simulated donation probabilities between treatment and control groups. - With a small sample size (n = 50), the sampling distribution of the difference in means is noisy and not very symmetric. - As the sample size increases (n = 200, 500, 1000), the distributions become tighter, smoother, and more centered around the true mean difference - In all cases, zero is near the center, not in the tail — suggesting no systematic bias in our estimator. This demonstrates why, in the real-world Karlan & List field experiment (with over 50,000 subjects!), we can reliably detect even small differences in donation behavior. The CLT ensures that the sampling distribution of the mean is approximately normal and centered, allowing valid statistical inference."
  },
  {
    "objectID": "blog/hw2.html",
    "href": "blog/hw2.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(tidyr)\n\n\nblueprinty &lt;- read.csv(\"blueprinty.csv\")\nhead(blueprinty)\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\nairbnb &lt;- read.csv(\"airbnb.csv\")\nhead(airbnb)\n\n  X   id days last_scraped host_since       room_type bathrooms bedrooms price\n1 1 2515 3130     4/2/2017   9/6/2008    Private room         1        1    59\n2 2 2595 3127     4/2/2017   9/9/2008 Entire home/apt         1        0   230\n3 3 3647 3050     4/2/2017 11/25/2008    Private room         1        1   150\n4 4 3831 3038     4/2/2017  12/7/2008 Entire home/apt         1        1    89\n5 5 4611 3012     4/2/2017   1/2/2009    Private room        NA        1    39\n6 6 5099 2981     4/2/2017   2/2/2009 Entire home/apt         1        1   212\n  number_of_reviews review_scores_cleanliness review_scores_location\n1               150                         9                      9\n2                20                         9                     10\n3                 0                        NA                     NA\n4               116                         9                      9\n5                93                         9                      8\n6                60                         9                      9\n  review_scores_value instant_bookable\n1                   9                f\n2                   9                f\n3                  NA                f\n4                   9                f\n5                   9                t\n6                   9                f\n\n\n\n# compare mean\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_patents = mean(patents),\n    sd_patents = sd(patents),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Table 1. Average Number of Patents by Customer Status\")\n\n\nTable 1. Average Number of Patents by Customer Status\n\n\niscustomer\nmean_patents\nsd_patents\ncount\n\n\n\n\n0\n3.473013\n2.225060\n1019\n\n\n1\n4.133056\n2.546846\n481\n\n\n\n\n# Compare Histogram\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30)+\n  labs(title = \"Patent Counts by Customer Status\",\n       x = \"Number of Patents\",\n       fill = \"Customer (1=Yes, 0=No)\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nExplanation: Blueprinty customers have a higher average number of patents(4.133) compare to non-customer(3.473) and from the histogram distribution, we can clearly observer that the Blueprinty customer tend to be more represented in the higher end of patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_age = round(mean(age), 1),\n    sd_age = round(sd(age), 2),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Table 2. Average Age by Customer Status\")\n\n\nTable 2. Average Age by Customer Status\n\n\niscustomer\nmean_age\nsd_age\ncount\n\n\n\n\n0\n26.1\n6.95\n1019\n\n\n1\n26.9\n7.81\n481\n\n\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer, region) %&gt;%\n  summarize(count = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = iscustomer, values_from = count, values_fill = 0) %&gt;%\n  kable(caption = \"Table 3. Customer Distribution by Region (0 = Non-Customer, 1 = Customer)\")\n\n\nTable 3. Customer Distribution by Region (0 = Non-Customer, 1 = Customer)\n\n\nregion\n0\n1\n\n\n\n\nMidwest\n187\n37\n\n\nNortheast\n273\n328\n\n\nNorthwest\n158\n29\n\n\nSouth\n156\n35\n\n\nSouthwest\n245\n52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: From the age comparison, we can observe that blueprinty customer has a higher average age which is 26.90021 compare with non-customer (26.10157). From the region comparison, we can observe that more blueprinty customer live in the Northeast and areas like Midwest, Northwest, South and Southwest have fever customers live in.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function for a Poisson random variable is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood function for the sample is:\n\\[\nL(\\boldsymbol{\\lambda}) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood is:\n\\[\n\\log L(\\boldsymbol{\\lambda}) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n\\]\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  ll &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n\n\n\n\n\n\n\nThe log-likelihood curve shows how the fit of the Poisson model varies with different values of lambda. The curve reaches a clear peak around λ = 3.8, indicating that this value maximizes the likelihood of observing the data. This is consistent with the idea that the maximum likelihood estimate (MLE) of lambda is the value that best explains the observed patent counts.\n\nlambda_mle &lt;- mean(blueprinty$patents)\nlambda_mle\n\n[1] 3.684667\n\n\nTaking the derivative of the log-likelihood and solving for λ gives us the result λ̂ = Ȳ. This makes intuitive sense because the Poisson distribution is parameterized by its mean. In our data, the average number of patents per firm is 3.685.\n\nneg_loglikelihood &lt;- function(lambda) {\n  -poisson_loglikelihood(lambda, blueprinty$patents)\n}\n\nmle_result &lt;- optim(par = 2, fn = neg_loglikelihood, method = \"Brent\", lower = 0.01, upper = 10)\n\n\nmle_result$par      # the best lambda \n\n[1] 3.684667\n\n-mle_result$value   # the largest lambda\n\n[1] -3367.684\n\n\nUsing the ‘optim()’, we numerically maximized the log-likelihood function and found that the MLE of λ is nearly 3.685. The value of the maximized log-likehood is approximately -3367. This high log-likelihood values indicates a better fitting model and can used for comparing models. ### Estimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)  \n  ll &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n                 Coefficient    Std_Error\n(Intercept)     -0.125735915 0.1122180354\nage              0.115793715 0.0063574230\nage_sq          -0.002228748 0.0000771291\nregionNortheast -0.024556782 0.0433762879\nregionNorthwest -0.034827790 0.0529311002\nregionSouth     -0.005441860 0.0524007440\nregionSouthwest -0.037784109 0.0471722463\niscustomer       0.060665584 0.0320588299\n\n\nFrom the output above, we can observe that age plays a positive and significant effect on patent output, but the negative coefficient on age squared suggests diminishing return age. Regional effects are small and mostly insignificant compared to the baseline Midwest. Plus, being as a blueprinty customer is associated with a 6.3% increase in the expected number of patents.\n\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                 data = blueprinty,\n                 family = poisson())\n\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe checked our results using R’s glm() function, which estimates the Poisson regression model with the same specification as our custom MLE implementation. The model includes a constant, firm age, age squared, region dummies (Midwest as the reference category), and a binary indicator for whether the firm is a Blueprinty customer.\nThe results from glm() are generally consistent with those obtained via optim(). While there are some differences in the magnitude of the coefficients—especially for the intercept and the customer indicator—this is likely due to slight differences in how the design matrix is constructed and how categorical variables are handled internally by glm(). Importantly, the direction and significance of the main variables remain consistent.\nThe coefficient on age is positive and significant, suggesting that older firms tend to have more patents. However, the negative and significant coefficient on age squared indicates diminishing returns to age: patent output increases with age up to a point, but the effect eventually tapers off.\nThe coefficient on the customer indicator is positive (0.208) and statistically significant (p &lt; 0.001), implying that, on average, Blueprinty customers have higher patent counts than non-customers. Exponentiation the coefficient gives:\n[ (0.208) ]\nThis means that, all else equal, Blueprinty customers are expected to have approximately 23.1% more patents than comparable non-customers.\nRegional effects are small and not statistically significant, suggesting limited explanatory power after controlling for firm characteristics.\nOverall, the glm() results confirm our earlier findings and validate the implementation of the custom MLE function.\n\nX_0 &lt;- blueprinty\nX_1 &lt;- blueprinty\n\nX_0$iscustomer &lt;- 0\nX_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(glm_model, newdata = X_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_model, newdata = X_1, type = \"response\")\n\ndelta &lt;- y_pred_1 - y_pred_0\n\nmean(delta)\n\n[1] 0.7927681\n\n\nTo better interpret the effect of Blueprinty’s software, we created two counterfactual datasets: one where no firm is treated (iscustomer = 0) and one where all firms are treated (iscustomer = 1). We used the fitted glm() model to predict the number of patents under each scenario. On average, firms are predicted to have approximately 0.793 if they are Blueprinty customers compared to if they are not. This suggests that the Blueprinty software has a meaningful and positive effect on patent output."
  },
  {
    "objectID": "blog/hw2.html#blueprinty-case-study",
    "href": "blog/hw2.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(tidyr)\n\n\nblueprinty &lt;- read.csv(\"blueprinty.csv\")\nhead(blueprinty)\n\n  patents    region  age iscustomer\n1       0   Midwest 32.5          0\n2       3 Southwest 37.5          0\n3       4 Northwest 27.0          1\n4       3 Northeast 24.5          0\n5       3 Southwest 37.0          0\n6       6 Northeast 29.5          1\n\nairbnb &lt;- read.csv(\"airbnb.csv\")\nhead(airbnb)\n\n  X   id days last_scraped host_since       room_type bathrooms bedrooms price\n1 1 2515 3130     4/2/2017   9/6/2008    Private room         1        1    59\n2 2 2595 3127     4/2/2017   9/9/2008 Entire home/apt         1        0   230\n3 3 3647 3050     4/2/2017 11/25/2008    Private room         1        1   150\n4 4 3831 3038     4/2/2017  12/7/2008 Entire home/apt         1        1    89\n5 5 4611 3012     4/2/2017   1/2/2009    Private room        NA        1    39\n6 6 5099 2981     4/2/2017   2/2/2009 Entire home/apt         1        1   212\n  number_of_reviews review_scores_cleanliness review_scores_location\n1               150                         9                      9\n2                20                         9                     10\n3                 0                        NA                     NA\n4               116                         9                      9\n5                93                         9                      8\n6                60                         9                      9\n  review_scores_value instant_bookable\n1                   9                f\n2                   9                f\n3                  NA                f\n4                   9                f\n5                   9                t\n6                   9                f\n\n\n\n# compare mean\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_patents = mean(patents),\n    sd_patents = sd(patents),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Table 1. Average Number of Patents by Customer Status\")\n\n\nTable 1. Average Number of Patents by Customer Status\n\n\niscustomer\nmean_patents\nsd_patents\ncount\n\n\n\n\n0\n3.473013\n2.225060\n1019\n\n\n1\n4.133056\n2.546846\n481\n\n\n\n\n# Compare Histogram\nggplot(blueprinty, aes(x = patents, fill = as.factor(iscustomer))) +\n  geom_histogram(position = \"identity\", alpha = 0.5, bins = 30)+\n  labs(title = \"Patent Counts by Customer Status\",\n       x = \"Number of Patents\",\n       fill = \"Customer (1=Yes, 0=No)\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nExplanation: Blueprinty customers have a higher average number of patents(4.133) compare to non-customer(3.473) and from the histogram distribution, we can clearly observer that the Blueprinty customer tend to be more represented in the higher end of patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nblueprinty %&gt;%\n  group_by(iscustomer) %&gt;%\n  summarize(\n    mean_age = round(mean(age), 1),\n    sd_age = round(sd(age), 2),\n    count = n()\n  ) %&gt;%\n  kable(caption = \"Table 2. Average Age by Customer Status\")\n\n\nTable 2. Average Age by Customer Status\n\n\niscustomer\nmean_age\nsd_age\ncount\n\n\n\n\n0\n26.1\n6.95\n1019\n\n\n1\n26.9\n7.81\n481\n\n\n\n\n\n\nblueprinty %&gt;%\n  group_by(iscustomer, region) %&gt;%\n  summarize(count = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = iscustomer, values_from = count, values_fill = 0) %&gt;%\n  kable(caption = \"Table 3. Customer Distribution by Region (0 = Non-Customer, 1 = Customer)\")\n\n\nTable 3. Customer Distribution by Region (0 = Non-Customer, 1 = Customer)\n\n\nregion\n0\n1\n\n\n\n\nMidwest\n187\n37\n\n\nNortheast\n273\n328\n\n\nNorthwest\n158\n29\n\n\nSouth\n156\n35\n\n\nSouthwest\n245\n52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation: From the age comparison, we can observe that blueprinty customer has a higher average age which is 26.90021 compare with non-customer (26.10157). From the region comparison, we can observe that more blueprinty customer live in the Northeast and areas like Midwest, Northwest, South and Southwest have fever customers live in.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nThe probability mass function for a Poisson random variable is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood function for the sample is:\n\\[\nL(\\boldsymbol{\\lambda}) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood is:\n\\[\n\\log L(\\boldsymbol{\\lambda}) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n\\]\n\npoisson_loglikelihood &lt;- function(lambda, Y) {\n  ll &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n\n\n\n\n\n\n\nThe log-likelihood curve shows how the fit of the Poisson model varies with different values of lambda. The curve reaches a clear peak around λ = 3.8, indicating that this value maximizes the likelihood of observing the data. This is consistent with the idea that the maximum likelihood estimate (MLE) of lambda is the value that best explains the observed patent counts.\n\nlambda_mle &lt;- mean(blueprinty$patents)\nlambda_mle\n\n[1] 3.684667\n\n\nTaking the derivative of the log-likelihood and solving for λ gives us the result λ̂ = Ȳ. This makes intuitive sense because the Poisson distribution is parameterized by its mean. In our data, the average number of patents per firm is 3.685.\n\nneg_loglikelihood &lt;- function(lambda) {\n  -poisson_loglikelihood(lambda, blueprinty$patents)\n}\n\nmle_result &lt;- optim(par = 2, fn = neg_loglikelihood, method = \"Brent\", lower = 0.01, upper = 10)\n\n\nmle_result$par      # the best lambda \n\n[1] 3.684667\n\n-mle_result$value   # the largest lambda\n\n[1] -3367.684\n\n\nUsing the ‘optim()’, we numerically maximized the log-likelihood function and found that the MLE of λ is nearly 3.685. The value of the maximized log-likehood is approximately -3367. This high log-likelihood values indicates a better fitting model and can used for comparing models. ### Estimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\npoisson_regression_loglikelihood &lt;- function(beta, Y, X) {\n  lambda &lt;- exp(X %*% beta)  \n  ll &lt;- sum(-lambda + Y * log(lambda) - lgamma(Y + 1))\n  return(ll)\n}\n\n\n\n                 Coefficient    Std_Error\n(Intercept)     -0.125735915 0.1122180354\nage              0.115793715 0.0063574230\nage_sq          -0.002228748 0.0000771291\nregionNortheast -0.024556782 0.0433762879\nregionNorthwest -0.034827790 0.0529311002\nregionSouth     -0.005441860 0.0524007440\nregionSouthwest -0.037784109 0.0471722463\niscustomer       0.060665584 0.0320588299\n\n\nFrom the output above, we can observe that age plays a positive and significant effect on patent output, but the negative coefficient on age squared suggests diminishing return age. Regional effects are small and mostly insignificant compared to the baseline Midwest. Plus, being as a blueprinty customer is associated with a 6.3% increase in the expected number of patents.\n\nglm_model &lt;- glm(patents ~ age + I(age^2) + region + iscustomer,\n                 data = blueprinty,\n                 family = poisson())\n\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe checked our results using R’s glm() function, which estimates the Poisson regression model with the same specification as our custom MLE implementation. The model includes a constant, firm age, age squared, region dummies (Midwest as the reference category), and a binary indicator for whether the firm is a Blueprinty customer.\nThe results from glm() are generally consistent with those obtained via optim(). While there are some differences in the magnitude of the coefficients—especially for the intercept and the customer indicator—this is likely due to slight differences in how the design matrix is constructed and how categorical variables are handled internally by glm(). Importantly, the direction and significance of the main variables remain consistent.\nThe coefficient on age is positive and significant, suggesting that older firms tend to have more patents. However, the negative and significant coefficient on age squared indicates diminishing returns to age: patent output increases with age up to a point, but the effect eventually tapers off.\nThe coefficient on the customer indicator is positive (0.208) and statistically significant (p &lt; 0.001), implying that, on average, Blueprinty customers have higher patent counts than non-customers. Exponentiation the coefficient gives:\n[ (0.208) ]\nThis means that, all else equal, Blueprinty customers are expected to have approximately 23.1% more patents than comparable non-customers.\nRegional effects are small and not statistically significant, suggesting limited explanatory power after controlling for firm characteristics.\nOverall, the glm() results confirm our earlier findings and validate the implementation of the custom MLE function.\n\nX_0 &lt;- blueprinty\nX_1 &lt;- blueprinty\n\nX_0$iscustomer &lt;- 0\nX_1$iscustomer &lt;- 1\n\ny_pred_0 &lt;- predict(glm_model, newdata = X_0, type = \"response\")\ny_pred_1 &lt;- predict(glm_model, newdata = X_1, type = \"response\")\n\ndelta &lt;- y_pred_1 - y_pred_0\n\nmean(delta)\n\n[1] 0.7927681\n\n\nTo better interpret the effect of Blueprinty’s software, we created two counterfactual datasets: one where no firm is treated (iscustomer = 0) and one where all firms are treated (iscustomer = 1). We used the fitted glm() model to predict the number of patents under each scenario. On average, firms are predicted to have approximately 0.793 if they are Blueprinty customers compared to if they are not. This suggests that the Blueprinty software has a meaningful and positive effect on patent output."
  },
  {
    "objectID": "blog/hw2.html#airbnb-case-study",
    "href": "blog/hw2.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n# check for missing values\ncolSums(is.na(airbnb))\n\n                        X                        id                      days \n                        0                         0                         0 \n             last_scraped                host_since                 room_type \n                        0                        35                         0 \n                bathrooms                  bedrooms                     price \n                      160                        76                         0 \n        number_of_reviews review_scores_cleanliness    review_scores_location \n                        0                     10195                     10254 \n      review_scores_value          instant_bookable \n                    10256                         0 \n\ndf_model &lt;- airbnb %&gt;%\n  filter(!is.na(number_of_reviews),\n         !is.na(price),\n         !is.na(room_type),\n         !is.na(review_scores_cleanliness),\n         !is.na(review_scores_location),\n         !is.na(review_scores_value),\n         !is.na(instant_bookable))\n\nWe began by examining the extent of missing data in the key variables. Three review score variables—review_scores_cleanliness, review_scores_location, and review_scores_value—each have over 10,000 missing entries, accounting for approximately 25% of the full dataset. Since these variables are essential for our regression model, we restrict the analysis to complete cases across all relevant fields.\n\n\n\n\n\n\n\n\n\nThe distribution of number_of_reviews is highly right-skewed. A large majority of listings have relatively few reviews, with the mode centered near zero.\n\ndf_model %&gt;%\n  group_by(room_type) %&gt;%\n  summarize(mean_reviews = mean(number_of_reviews), .groups = \"drop\")\n\n# A tibble: 3 × 2\n  room_type       mean_reviews\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 Entire home/apt         21.4\n2 Private room            21.4\n3 Shared room             17.1\n\n\nWe compare the average number of reviews across listing types. Both entire apartments and private rooms receive, on average, around 21 reviews per listing, while shared rooms receive noticeably fewer, averaging just over 17. This suggests that listings offering greater privacy may be more attractive to potential guests, potentially leading to a higher volume of bookings and therefore more reviews.\n\n\n\nCall:\nglm(formula = number_of_reviews ~ price + room_type + review_scores_cleanliness + \n    review_scores_location + review_scores_value + instant_bookable, \n    family = poisson(), data = df_model)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.568e+00  1.529e-02 233.370   &lt;2e-16 ***\nprice                     -7.422e-06  7.573e-06  -0.980    0.327    \nroom_typePrivate room     -2.524e-02  2.683e-03  -9.406   &lt;2e-16 ***\nroom_typeShared room      -2.648e-01  8.562e-03 -30.926   &lt;2e-16 ***\nreview_scores_cleanliness  1.130e-01  1.484e-03  76.151   &lt;2e-16 ***\nreview_scores_location    -8.213e-02  1.586e-03 -51.797   &lt;2e-16 ***\nreview_scores_value       -9.003e-02  1.788e-03 -50.354   &lt;2e-16 ***\ninstant_bookablet          3.324e-01  2.877e-03 115.542   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 971266  on 30345  degrees of freedom\nResidual deviance: 947830  on 30338  degrees of freedom\nAIC: 1070159\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe estimate a Poisson regression model to understand how listing characteristics are associated with the number of reviews, which we use as a proxy for booking volume. The dependent variable is number_of_reviews, and predictors include price, room type, review scores, and instant bookability.\nFor the room type, private room received 2.5% fewer reviews while the share room received 23% fewer reviews, holding other variables constant.\nFor the review scores, as the cleanliness increased by one point, the reviews increased by 12%. However, As values and location increased by 1 unit, the reviews decreased 8-9%, holding other variables constant.\nFor the instant booking, the hotels that can instantly booked reviews received 39% more than hotels that need hotel approvals.\nFor price, this variable is statistically significant in the model.\nThese results highlight that convenience and perceived cleanliness matter more than price, and that shared accommodations may be less attractive to guests based on actual review behavior."
  }
]